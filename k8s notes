abcdefgh
servies are of 4 types 
a) nodeport
b) clusterIP
c) loadbalancer
d) External Name

kubectl run dheeraj1 --image=nginx --port=80 --restart=Never -o yaml --- command to create pod and showing the out over the screen in yml format 

Services --- service ip is not reachable form outside 
every master nad slave is having ip address but a user can access only either master ip or worker ip 
master ip should be hidden to outside 
here comes Nodeport and Loadbalancer 
bcs every container is working on diffenret port so we can access via port forwarding in docker 

file to create pod with labels
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    x: hello
  name: dheeraj12
spec:
  containers:
  - image: nginx
    name: dheeraj12
    ports:
    - containerPort: 80
    
file to create service of nodeport type 
apiVersion: v1
kind: Service
metadata:
  name: myservice

spec: 
 ports:
 - name: myserport
   port: 11222
   targetPort: 80
 selector:
   x: hello
 type: NodePort


a) create a pod with lables and create service of nodeport type and it will generate genrate port and we can use this with master or node 


command to create service via cmd --- kubectl  create service  nodeport mypod2s1 --tcp 80 --dry-run -o yaml
   
 ******************************************* DAY 5 **********
 
 nodeprt range ---> 30k to 32767

best way to write POD and Service file in a single file  is 
***
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: ashupod1
  name: ashupod1
spec:
  containers:
  - image: nginx
    name: ashupod1
    ports:
    - containerPort: 80
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}
---
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: null
  labels:
    app: ashubestways1
  name: ashubestways1
spec:
  ports:
  - name: "80"
    port: 80
    protocol: TCP
    targetPort: 80
  selector:
    run: ashupod1
  type: NodePort
status:
  loadBalancer: {}
********

how to assign service to a running pods
kubectl expose pods ashupod1 --type NodePort --port 80 

how to get json or yaml file from a running pods
kubectl get svc servicename --export -o yaml

how to run command like ping in pod
command : ["/bin/sh", "-c","ping 8.8.8.8"] 

command can replace entrypoint which is written in Dockerfile 

*******************
 Replication Controller--- if pod got down in nod then it will schedule this over other node or in the same node ...
 its duty to maintain the count of pod 
 apiVersion: v1
kind: ReplicationController
metadata:
  name: myrcashu1
  labels:
    app: web1 #label for rc not for pod
spec:
  replicas: 1 #always one pod will be present even in case of current worker node failure
  template:
    metadata:
      name: mypod11 # this will be the podname that will be created by RC
      labels:
        x: helloadhoc
    spec:
      containers:
      - name: ashux11
        image: nginx
        ports:
        - containerPort: 80
kubectl delete all -all ---it will all of the thing we are having at the same time

***************
VOLUME
**********
add extra storage and integrate with pods

catagory of volumes
a) hostpath --- when u mount a location of pod of worker node to system ---doubt little bit 
using portainer we can manage all the pod in a node
first we need to create a volume for that and after expose service for that container
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: myportal1
  name: myportal1
spec:
  volumes:
  - name: dheerajvol1
    hostPath:
      path: /var/run/docker.sock
      type: Socket

  nodeName: myrcashu2-77zlp
  containers:
  - image: portainer/portainer
    name: myportal1
    ports:
    - containerPort: 9000
    volumeMounts:
    - name: dheerajvol1
      mountPath: /var/run/docker.sock
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}

b) EmptyDir --- storgae speed is fast but temporary 
k8s supports cloud storage as well like EBS in azure
 volume file 
 #########################
 apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: ashupod1
    x: hello
  name: ashupod1
spec:
  volumes: #this is for creating volume of any type
  - name: ashuvol111 # name of volume
    emptyDir : {} # it will create a temp and random folder in worker node after deleting this it will also get deleted
  containers:
  - image: alpine
    name: ashupod1
    volumeMounts:
    - name: ashuvol111
      mountPath: /mnt/date/ # this folder will get create automatically for mounting purpose
    ports:
    - containerPort: 80
    command: ["/bin/sh","while true ; do date >> /mnt/date/output.txt; sleep 2; done"]
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {}


hostPath volume type
you can create multiple volumes and mount them in single file 

******************
Q) how to access a pod without creating any services 
***************************
what is portainer
**************


we need to download admin.conf file at our server where kubectl is running under config folder 

Namespaces ---> 

if we are using the same admin.conf file for different node then the pods they have deployed can acess and see each others node's pod details in a cluster
so if we want to hide the data of each node then we can create namespace on top of the cluster .
by default when kubernetes get installed default namespase is there.

by default there are 4 types of namspases.
a) default -- > when u r getting connected with the cluster this will come
b) kube-system -- > any service will be stored in this in the form of pod like etcd
c) kube-public -- > its like the template
d) kube-node-lease -- > it is for backup, maintainance...

command to create namespaces --- > kubectl create namespace adhicns1

same pod can be created in different namespaces 

kubectl replace -f /tmp/ns.yml --force -n adhoc ---> command to delete and recreate the pod 

couple of resources which are not boundedd with namespaces ..> like nodes, namespace, 
to see ---> kubectl api-resources

****************************

difference bw Replication controller and replica set ?

replica set is not being used anymore bcs if you shift to new from old contianer will get delete and this will create downtime which is not suitable.

to do updation we use rolling updation by stop taking new connection for a pod and when serving old collection it will get deleted and will create new one 
so total count will be same at anytime 

**************
deployment
***************
command to create deployments -- > kubectl create deployments ashudep1 --image=nginx --dry-run -o yaml
add service   --- > kubectl expose deployment ashudep1 --type Nodeport --port 80 -n dheerajns1

scale deployment ---> kubectl scale deployment ashudep1 --replicas=3 -n dheerajns

how to update app from v1 to v2 
deployments maintaine the version by revision number 

method --- > a) edit the file and change the image version of container
after making changes and apply file changes rolling update will done in the backend 

checking the rollout history ..--> it will show the revision number history 

command---> kubectl rollout history deployment ashudep1 -n dheerajns

command to downgrade the revision ---> kubectl rollout undo deployment ashudep1 -n dheerajns

command to go to specific revision number --> kubectl rollout undo deployment ashudp1 -n dheerajns --to-revision=1 












 
 
 
 

    
    

